{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (PSL) Project 3: Preprocessing Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code reads in Split 1 training data, fits a binary classifier model to it (Logistic Regression with ElasticNet and Standard Scaler), and pickles and saves the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "train = pd.read_csv('F24_Proj3_data/split_1/train.csv')\n",
    "X_train = train.iloc[:, 3:]\n",
    "y_train = train['sentiment']\n",
    "\n",
    "model_pipe = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(C=0.01, l1_ratio=0.1, penalty='elasticnet', \n",
    "                       solver='saga', max_iter=10000, random_state=5671))\n",
    "model_pipe.fit(X_train, y_train);\n",
    "\n",
    "# import pickle\n",
    "# with open('split_1_model.pkl', 'wb') as f:\n",
    "#     pickle.dump(model_pipe, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code reads in split 1 test data, runs the pretrained model on it to make predictions of positive sentiment for each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the Split 1 test data, use the pretrained model to predict probability of positive label for each review\n",
    "test = pd.read_csv('F24_Proj3_data/split_1/test.csv')\n",
    "test['review_pred_og'] = model_pipe.predict_proba(test.iloc[:, 2:])[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code takes a sample of test reviews and their OpenAI embeddings (1000 x 1536), and uses BERT model to generate corresponsing BERT embeddings (1000 x 768). Then, perform linear regression to approximate Open AI embeddings from BERT embeddings, save the transformation matrix CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load distilBERT, pretrained tokenizer and model\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "bert_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Sample test data, extract the 1536 OpenAI embeddings\n",
    "np.random.seed(287)\n",
    "samp_rev_ids = np.random.choice(test['id'], size=10000, replace=False)\n",
    "sample_test = test.loc[test['id'].isin(samp_rev_ids), :]\n",
    "sample_test_openai = sample_test.drop(columns=['id','review','review_pred_og'])\n",
    "\n",
    "# For each review in the sample, tokenize and convert to BERT embeddings\n",
    "sample_test_bert_matrix = []\n",
    "for rev in sample_test['review']:\n",
    "    these_tokens = bert_tokenizer(rev, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        this_output = bert_model(**these_tokens)\n",
    "        this_emb = this_output.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "    sample_test_bert_matrix.append(this_emb.flatten())\n",
    "\n",
    "sample_test_bert = pd.DataFrame(np.array(sample_test_bert_matrix))\n",
    "\n",
    "\n",
    "# Fit a linear regression model to approximate OpenAI embeddings from Bert embeddings\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "W_model = LinearRegression()\n",
    "W_model.fit(sample_test_bert, sample_test_openai)\n",
    "# The transformation matrix W are the model's coefficients\n",
    "W = W_model.coef_.T  # Shape will be (768, 1536)\n",
    "\n",
    "# # Save csv\n",
    "# mysubmission_df = pd.DataFrame(W)\n",
    "# mysubmission_df.to_csv('W.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code samples 5 positive and 5 negative reviews with their original OpenAI embeddings, from Split 1 test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# From Split 1 test data, list review ids of positive \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# labels (> 0.5 prob of positive), and negative (otherwise)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m pos_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[38;5;241m.\u001b[39mloc[test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_pred_og\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m neg_ids \u001b[38;5;241m=\u001b[39m test\u001b[38;5;241m.\u001b[39mloc[test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_pred_og\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Set random seed, and sample 5 pos and 5 neg reviews\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "# From Split 1 test data, list review ids of positive \n",
    "# labels (> 0.5 prob of positive), and negative (otherwise)\n",
    "pos_ids = test.loc[test['review_pred_og'] > 0.5, 'id']\n",
    "neg_ids = test.loc[test['review_pred_og'] < 0.5, 'id']\n",
    "\n",
    "# Set random seed, and sample 5 pos and 5 neg reviews\n",
    "np.random.seed(287)\n",
    "pos_ids_sample = np.random.choice(pos_ids, size=5, replace=False)\n",
    "neg_ids_sample = np.random.choice(neg_ids, size=5, replace=False)\n",
    "\n",
    "# A sample of 10 reviews, 5 predicted positive and 5 negative\n",
    "ids_sample = np.concatenate([pos_ids_sample, neg_ids_sample])\n",
    "interp_df_full = test.loc[test['id'].isin(ids_sample), :]\n",
    "\n",
    "# # Save to csv\n",
    "# interp_df_full.to_csv('sample_reviews.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code splits the 10 sample reviews into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate embeddings matrix (10 x 1536) and other columns\n",
    "interp_df_only_embed = interp_df_full.drop(columns=['id', 'review', 'review_pred_og'])\n",
    "interp_df_no_embed = interp_df_full[['id', 'review', 'review_pred_og']]\n",
    "\n",
    "import re\n",
    "# Function to split review into sentences\n",
    "def by_sentence(review):\n",
    "    return re.split(r'(?<=[.!?])\\s*(?=\\w)', review)\n",
    "\n",
    "# From reviews, remove <br> and extra spaces\n",
    "interp_df_no_embed['review'] = interp_df_no_embed['review'].str.replace(r'<.*?>', ' ', regex=True).str.strip()\n",
    "# Apply split function to 'review' column\n",
    "interp_df_no_embed['sentence'] = interp_df_no_embed['review'].apply(by_sentence)\n",
    "# Explode the 'sentence' column (each row is a sentence), remove extra spaces\n",
    "interp_df_split = interp_df_no_embed.explode('sentence')\n",
    "interp_df_split['sentence'] = interp_df_split['sentence'].str.strip()\n",
    "\n",
    "# interp_df_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code converts each sample sentence to BERT embeddings, then uses the precalculated conversion matrix to convert the BERT embeddings to approximate OpenAI embeddings, aligning them with the original embedding format and dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load distilBERT, pretrained tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "emb_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "sentences_to_bert_matrix = []\n",
    "\n",
    "# Iterate over each sentence in sample reviews\n",
    "for sen in interp_df_split['sentence']:\n",
    "    # Tokenize this sentence\n",
    "    this_input = tokenizer(sen, return_tensors=\"pt\", \n",
    "                       padding=True, truncation=True) # Control input token size\n",
    "\n",
    "    # Generate embeddings, average over tokens to get fixed-length\n",
    "    with torch.no_grad():\n",
    "        this_output = emb_model(**this_input)\n",
    "        this_emb = this_output.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "    # Flatten and append embedding to output list\n",
    "    sentences_to_bert_matrix.append(this_emb.flatten())\n",
    "\n",
    "# The matrix of 768 BERT embeddings, with a row for each sample sentence\n",
    "sentences_to_bert = pd.DataFrame(np.array(sentences_to_bert_matrix))\n",
    "\n",
    "\n",
    "\n",
    "# Use saved W matrix to convert this df from 768 BERT embeddings to 1536 OpenAI embeddings\n",
    "sentences_to_openai = sentences_to_bert @ W\n",
    "# Run the pretrained model to predict probability of positive sentiment\n",
    "model_pipe.predict_proba(sentences_to_openai)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
